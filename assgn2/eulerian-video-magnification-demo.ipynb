{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例简介\n",
    "## 欧拉放大介绍\n",
    "EVM(Eulerian Video Magnification,欧拉视频放大或欧拉影像放大)是一种将视频中的微小变化进行放大的算法，该算法可以将视频中的微小变化转变为肉眼可以观察的变化。这个算法可以应用在从视频中提取心率信息，也可以放大视频中细小的运动变化。\n",
    "## Spatial Decomposition\n",
    "将视频中的每一帧进行下采样，形成图像金字塔。这里图像金字塔有两种组件方式，一种是高斯金字塔，还有一种是拉普拉斯金字塔。高斯金字塔常常用于放大色彩，而拉普拉斯金字塔用于放大运动。\n",
    "## Temporal Filter\n",
    "视频可以看做连续的图片，从图片中的单个像素点的角度看，视频每个像素点的变化可以看成时域信号。而物体运动的信息就隐藏在单个像素点的变化之中。论文采用的方法是进行带通滤波（bandpass filter），放大颜色的时候采用理想带通滤波器，放大运动的时候采用IIR滤波器，论文中主要使用了巴特沃斯滤波器（Butterworth filter）。\n",
    "## Reconstraction\n",
    "最后一步流程就是将分解成图像金字塔的图片再复原回去。这篇论文中，高斯金字塔的复原方法是将高斯金字塔中最低一级（图片最小的那张）进行上采样，然后叠加到原图中。在工程实现中，最后图片的像素值会超过255，发生溢出，此时需要做的是把像素值归一化到0-255之间。应该注意到，这里与原论文的描述不完全一样。对于拉普拉斯金字塔，先将金字塔最低一级进行上采样，然后与上一级进行叠加，以此类推，最后与原图叠加，这里的方法与论文本身基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取视频并分离每一帧数据，由于内存问题这里可以限制视频的帧数\n",
    "def separateVideoFrameToDisk(videoname, path, limitedCount = 300):\n",
    "    frames = [ ]\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    cap = cv2.VideoCapture(videoname)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    ret, image = cap.read()\n",
    "    count = 0\n",
    "    ret = True\n",
    "    while ret and count < limitedCount:\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #cv2.imwrite(os.path.join(path, \"frame%d.jpg\" % count), image)\n",
    "        frames.append(image)\n",
    "        ret, image = cap.read()\n",
    "        count += 1\n",
    "    frames = np.array(frames)\n",
    "    cap.release()\n",
    "    return fps, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析视频为图片\n",
    "#fps, frames = separateVideoFrameToDisk('data/face.mp4', 'frames/face')\n",
    "fps, frames = separateVideoFrameToDisk('data/baby2.mp4', 'frames/baby2')\n",
    "#fps, frames = separateVideoFrameToDisk('data/baby.mp4', 'frames/baby', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameNum, H, W, chNum = frames.shape\n",
    "print(frameNum, W, H, chNum)\n",
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出某个像素时域信号变化情况\n",
    "def draw1DTemporalSignal(frames, ypos, xpos, fps = 30):\n",
    "    frameNum, H, W, chNum = frames.shape\n",
    "    tlist = [t * 1000 / fps for t in range(frameNum)]\n",
    "    #print(tlist)\n",
    "    #print(len(frames[:, ypos, xpos, 2]))\n",
    "    plt.figure(figsize=(20,5))\n",
    "    channelName=['R', 'G', 'B']\n",
    "    for c in range(chNum):\n",
    "        plt.subplot(1, 3, c+1)\n",
    "        plt.plot(tlist, frames[:, ypos, xpos, c], '-g')\n",
    "        plt.title('%s channel pixel value change for point (%d, %d)' % (channelName[c], ypos, xpos))\n",
    "        plt.xlabel('Time(ms)')\n",
    "        plt.ylabel('Pixel Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw1DTemporalSignal(frames, 100, 100, fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map所有的视频帧像素值到[0..1]，归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化操作\n",
    "def normalization(x,Max,Min):\n",
    "    y = np.ndarray(shape=x.shape, dtype=np.float64)\n",
    "    y = (x - Min) / (Max - Min);\n",
    "    return y\n",
    "\n",
    "frames_normal = np.ndarray(shape=frames.shape, dtype=np.float64)\n",
    "for frame,n in zip(frames, range(frameNum)):\n",
    "    img = normalization(frame, 255, 0)\n",
    "    frames_normal[n] = img\n",
    "del frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert RGB to YIQ\n",
    "def rgb2ntsc(R,G,B):\n",
    "    Y = 0.299*R + 0.587*G + 0.114*B\n",
    "    I = 0.596*R - 0.274*G - 0.322*B\n",
    "    Q = 0.211*R - 0.523*G + 0.312*B\n",
    "    return Y,I,Q\n",
    "\n",
    "#convert YIQ to RGB\n",
    "def ntsc2rgb(Y,I,Q):\n",
    "    R = 1.000*Y + 0.956*I + 0.621*Q\n",
    "    G = 1.000*Y + -0.272*I + -0.647*Q\n",
    "    B = 1.000*Y + -1.106*I + 1.703*Q\n",
    "    return R,G,B\n",
    "\n",
    "def convert_RGB_frame_to_YIQ(frame):\n",
    "    dtype = frame.dtype\n",
    "    result = np.ndarray(shape=frame.shape, dtype=dtype)\n",
    "    np.copyto(result, frame)\n",
    "    #R,G,B = extract_tuples(frame)\n",
    "    R = frame[:,:,0]\n",
    "    G = frame[:,:,1]\n",
    "    B = frame[:,:,2]\n",
    "    Y,I,Q = rgb2ntsc(R,G,B)\n",
    "    result[:,:,0] = Y\n",
    "    result[:,:,1] = I\n",
    "    result[:,:,2] = Q\n",
    "    return result\n",
    "\n",
    "def convert_YIQ_frame_to_RGB(frame):\n",
    "    dtype = frame.dtype\n",
    "    result = np.ndarray(shape=frame.shape, dtype=dtype)\n",
    "    np.copyto(result, frame)\n",
    "    Y = frame[:,:,0]\n",
    "    I = frame[:,:,1]\n",
    "    Q = frame[:,:,2]\n",
    "    R,G,B = ntsc2rgb(Y,I,Q)\n",
    "    result[:,:,0] = R \n",
    "    result[:,:,1] = G \n",
    "    result[:,:,2] = B \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证YIQ转换是否能逆变换回RGB图\n",
    "def checkYIQTransform(YIQimg):\n",
    "    RGBImg = convert_YIQ_frame_to_RGB(YIQimg)\n",
    "    print(RGBimg)\n",
    "    RGBImg *= 255\n",
    "    RGBImg=RGBImg.astype(np.uint8)\n",
    "    print(RGBimg)\n",
    "    RGBImg[RGBimg>255] = 255\n",
    "    RGBImg[RGBimg<0] = 0\n",
    "    cv2.imwrite(os.path.join('frames', \"rgb.jpg\" ), RGBimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 转换所有的视频frame到YIQ色彩空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame,i in zip(frames_normal, range(frameNum)):\n",
    "    img = convert_RGB_frame_to_YIQ(frame)\n",
    "    frames_normal[i] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 拉普拉斯金字塔构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lappyr(frameYIQ, leveln = 4):\n",
    "    lappyrs = []\n",
    "    img = frameYIQ\n",
    "    for i in range(leveln-1):\n",
    "        next_img = cv2.pyrDown(img)\n",
    "        (h, w) = img.shape[:2]\n",
    "        img1 = cv2.pyrUp(next_img, dstsize=(w, h))\n",
    "        #print((img-img1).shape)\n",
    "        lappyrs.append(img-img1)\n",
    "        img = next_img\n",
    "    #print(img.shape)\n",
    "    lappyrs.append(img)\n",
    "    return np.array(lappyrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_video(frames,levels=4):\n",
    "    print(\"  laplacian_video E\")\n",
    "    lappyrAll=[]\n",
    "    for i in range(0, frames.shape[0]):\n",
    "        frame=frames[i]\n",
    "        pyr=build_lappyr(frame,levels)\n",
    "        for n in range(levels):\n",
    "            if i==0:\n",
    "                lappyrAll.append(np.zeros((frames.shape[0], pyr[n].shape[0], pyr[n].shape[1],3), dtype=\"float64\"))\n",
    "            lappyrAll[n][i] = pyr[n]\n",
    "    print(\"  laplacian_video X\")\n",
    "    return lappyrAll\n",
    "\n",
    "lappyrAll = laplacian_video(frames_normal)\n",
    "del frames_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 带通滤波器实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack as fftpack\n",
    "def temporal_bandpass_filter(data, fps, freq_min=0.833, freq_max=1, axis=0, amplification_factor=1):\n",
    "    \"\"\"Found from https://github.com/brycedrennan/eulerian-magnification. Will expand later.\"\"\"\n",
    "    fft = fftpack.rfft(data, axis=axis)\n",
    "    frequencies = fftpack.fftfreq(data.shape[0], d=1.0 / fps)\n",
    "    bound_low = (np.abs(frequencies - freq_min)).argmin()\n",
    "    bound_high = (np.abs(frequencies - freq_max)).argmin()\n",
    "    fft[:bound_low] = 0 \n",
    "    fft[bound_high:-bound_high] = 0 \n",
    "    fft[-bound_low:] = 0 \n",
    "\n",
    "    result = np.ndarray(shape=data.shape, dtype='float')\n",
    "    result[:] = fftpack.ifft(fft, axis=0)\n",
    "    result *= amplification_factor\n",
    "    return result\n",
    "\n",
    "def filter_lap_pyramid(video_pyramid, fps, freq_min=0.833, freq_max=1, axis=0, amplification_factor=1):\n",
    "    print(\"filter_lap_pyramid E\")\n",
    "    #print(\"video_pyramid = \", len(video_pyramid))\n",
    "    for i in range(0, len(video_pyramid)):\n",
    "        video = video_pyramid[i]\n",
    "        filtered_video = temporal_bandpass_filter(video, fps, freq_min, freq_max, axis, amplification_factor)\n",
    "        video_pyramid[i] += filtered_video\n",
    "        print(filtered_video.shape)\n",
    "    print(\"filter_lap_pyramid X\")\n",
    "    return video_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredPyrs = filter_lap_pyramid(lappyrAll, fps, 0.4, 3, 0, 10)\n",
    "del lappyrAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 利用过滤并完成放大的拉布拉斯金字塔从新还原视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstract_video(filteredPyramid, pyramidLevels = 4):\n",
    "    print(\"reconstructePyramid E\")\n",
    "    level = pyramidLevels - 1\n",
    "    while ( level >= 1):\n",
    "        for frameNum in range(len(filteredPyramid[level])):\n",
    "            filteredPyramid[level - 1][frameNum] = cv2.pyrUp(filteredPyramid[level][frameNum]) + filteredPyramid[level - 1][frameNum]\n",
    "        level -= 1\n",
    "    reconstructedData = np.array(filteredPyramid[0])\n",
    "\n",
    "    print(\"reconstructePyramid X\")\n",
    "    return reconstructedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newYIQFrames = reconstract_video(filteredPyrs)\n",
    "del filteredPyrs\n",
    "newRGBFrames = np.ndarray(shape=newYIQFrames.shape, dtype=np.float64)\n",
    "for frame,i in zip(newYIQFrames, range(frameNum)):\n",
    "    img = convert_YIQ_frame_to_RGB(frame)\n",
    "    newRGBFrames[i] = img\n",
    "del newYIQFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 保存还原后的frame为结果视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveVideo(frames, output,width,height):\n",
    "    print(\"saveVideo E\")\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    writer = cv2.VideoWriter(\"result/\"+output+\".avi\", fourcc, 30, (width, height), 1)\n",
    "    for frame in frames:\n",
    "        frame *= 255\n",
    "        frame[frame>255] = 255\n",
    "        frame[frame<0] = 0\n",
    "        writer.write(cv2.convertScaleAbs(frame))\n",
    "    writer.release()\n",
    "    print(\"saveVideo X\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveVideo(newRGBFrames, \"face\", W, H)\n",
    "saveVideo(newRGBFrames, \"baby2\", W, H)\n",
    "#saveVideo(newRGBFrames, \"baby\", W, H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
